{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import librosa\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, LSTM\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data_cut/vika-shon-4d.wav</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data_cut/tamar-bedi-4bhigh.wav</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data_cut/nika-onia-1clow.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data_cut/nino-chan-3chigh.wav</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data_cut/mariam-aval-2chigh.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file_name  correct\n",
       "0        ../data_cut/vika-shon-4d.wav        4\n",
       "1   ../data_cut/tamar-bedi-4bhigh.wav        4\n",
       "2     ../data_cut/nika-onia-1clow.wav        1\n",
       "3    ../data_cut/nino-chan-3chigh.wav        3\n",
       "4  ../data_cut/mariam-aval-2chigh.wav        2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_audio = '../data_cut/'\n",
    "path_to_validation = '/test_c/'\n",
    "\n",
    "def_w, def_h = 0, 0 # Default width and height of spectogram images\n",
    "num_classes = 5\n",
    "skip = 5 # Useful in signal[skip::] to shrink data size, not necessary right now\n",
    "\n",
    "# Returns correct int from file name\n",
    "def parse_number(file_path):\n",
    "  return int(''.join(ch for ch in list(file_path) if ch.isdigit()))\n",
    "\n",
    "# Return list of tuples (file_path, correct number)\n",
    "def list_of_audios(dir_path):\n",
    "  arr = glob.glob(dir_path + '*.wav')\n",
    "  random.shuffle(arr) # Shuffled data is better for training\n",
    "  return list(map(lambda x: (x, parse_number(x)), arr))\n",
    "\n",
    "df = pd.DataFrame(list_of_audios(path_to_audio), columns = ['file_name', 'correct'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 analyzed out of 2040\n",
      "100 analyzed out of 2040\n",
      "150 analyzed out of 2040\n",
      "200 analyzed out of 2040\n",
      "250 analyzed out of 2040\n",
      "300 analyzed out of 2040\n",
      "350 analyzed out of 2040\n",
      "400 analyzed out of 2040\n",
      "450 analyzed out of 2040\n",
      "500 analyzed out of 2040\n",
      "550 analyzed out of 2040\n",
      "600 analyzed out of 2040\n",
      "650 analyzed out of 2040\n",
      "700 analyzed out of 2040\n",
      "750 analyzed out of 2040\n",
      "800 analyzed out of 2040\n",
      "850 analyzed out of 2040\n",
      "900 analyzed out of 2040\n",
      "950 analyzed out of 2040\n",
      "1000 analyzed out of 2040\n",
      "1050 analyzed out of 2040\n",
      "1100 analyzed out of 2040\n",
      "1150 analyzed out of 2040\n",
      "1200 analyzed out of 2040\n",
      "1250 analyzed out of 2040\n",
      "1300 analyzed out of 2040\n",
      "1350 analyzed out of 2040\n",
      "1400 analyzed out of 2040\n",
      "1450 analyzed out of 2040\n",
      "1500 analyzed out of 2040\n",
      "1550 analyzed out of 2040\n",
      "1600 analyzed out of 2040\n",
      "1650 analyzed out of 2040\n",
      "1700 analyzed out of 2040\n",
      "1750 analyzed out of 2040\n",
      "1800 analyzed out of 2040\n",
      "1850 analyzed out of 2040\n",
      "1900 analyzed out of 2040\n",
      "1950 analyzed out of 2040\n",
      "2000 analyzed out of 2040\n",
      "Different shapes: {(20, 23), (20, 25), (20, 24), (20, 34), (20, 31), (20, 20), (20, 29), (20, 28), (20, 37), (20, 17)}\n",
      "Every spectogram should be size of: (20, 37)\n"
     ]
    }
   ],
   "source": [
    "def audios_to_spectograms(file_names):\n",
    "  # Save different shapes in a set\n",
    "  x, shapes = [], set()\n",
    "\n",
    "  # Enumerate for logging\n",
    "  for indx, audio_file in enumerate(file_names):\n",
    "    # Use mfcc algorithm for spectograms\n",
    "    signal, sampling_rate = librosa.load(audio_file) \n",
    "    matrix = librosa.feature.mfcc(signal, sampling_rate)\n",
    "\n",
    "    x.append(matrix)\n",
    "    shapes.add(matrix.shape)\n",
    "    if (indx+1) % 50 == 0: print('{} analyzed out of {}'.format(indx+1, len(file_names))) # Log progress\n",
    "    \n",
    "  return x, shapes\n",
    "\n",
    "def choose_max_shapes(shapes):\n",
    "  # Iterate over shapes and choose biggest possible width and height\n",
    "  w, h = 0, 0\n",
    "  for shape in shapes:\n",
    "    w = max(w, shape[0])\n",
    "    h = max(h, shape[1])\n",
    "  return w, h\n",
    "\n",
    "matrices, shapes = audios_to_spectograms(df['file_name'])\n",
    "print('Different shapes:', shapes)\n",
    "def_w, def_h = choose_max_shapes(shapes)\n",
    "print('Every spectogram should be size of:', (def_w, def_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2040, 20, 37)\n"
     ]
    }
   ],
   "source": [
    "def pad_spectogram(matrix):\n",
    "  # Since width is always 20 in mfcc, we only check for height difference\n",
    "  if matrix.shape[1] < def_h:\n",
    "    diff = def_h - matrix.shape[1]\n",
    "    # Append half of the difference in beginning\n",
    "    matrix = np.append(np.zeros((matrix.shape[0], diff//2), dtype=float), matrix, axis=1)\n",
    "    #Append res in the end\n",
    "    matrix = np.append(matrix, np.zeros((matrix.shape[0], diff - diff//2), dtype=float), axis=1)\n",
    "  return matrix\n",
    "\n",
    "x = np.array([pad_spectogram(matrix) for matrix in matrices])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " ...\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# One hot encode correct numbers\n",
    "y = np.matrix([[0] * (num-1) + [1] + [0] * (num_classes - num) for num in df['correct'].values])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2040, 20, 37, 1) (2040, 5) (20, 37, 1)\n"
     ]
    }
   ],
   "source": [
    "x_r = x.reshape(*x.shape, 1)\n",
    "y_r = y.reshape(*y.shape, 1)\n",
    "#x_r = (x_r - x_r.mean()) / x_r.std()\n",
    "input_shape = x_r.shape[1:]\n",
    "print(x_r.shape, y_r.shape, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "                             \n",
    "      Conv2D(32, kernel_size = (2,2), activation = 'relu', input_shape = input_shape),\n",
    "      BatchNormalization(),\n",
    "\n",
    "      Conv2D(64, kernel_size=(2, 2), activation = 'relu'),\n",
    "      BatchNormalization(),\n",
    "\n",
    "      Conv2D(128, kernel_size=(2, 2), activation = 'relu'),\n",
    "      BatchNormalization(),\n",
    "\n",
    "      MaxPooling2D(pool_size = (2,2)),\n",
    "      BatchNormalization(),\n",
    "      Dropout(0.3),\n",
    "      Flatten(),\n",
    "\n",
    "      Dense(128, activation = 'relu'),\n",
    "      BatchNormalization(),\n",
    "      Dropout(0.2),\n",
    "\n",
    "      Dense(64, activation = 'relu'),\n",
    "      BatchNormalization(),\n",
    "      Dropout(0.25),\n",
    "\n",
    "      Dense(32, activation = 'relu'),\n",
    "      BatchNormalization(),\n",
    "\n",
    "      Dense(5, activation = 'softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 19, 36, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 19, 36, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 18, 35, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 18, 35, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 17, 34, 128)       32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 17, 34, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 17, 128)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 17, 128)        512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 17, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 17408)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               2228352   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 2,282,469\n",
      "Trainable params: 2,281,317\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1530 samples, validate on 510 samples\n",
      "Epoch 1/12\n",
      "1530/1530 [==============================] - 26s 17ms/step - loss: 1.3288 - acc: 0.4908 - val_loss: 0.8780 - val_acc: 0.6667\n",
      "Epoch 2/12\n",
      "1530/1530 [==============================] - 14s 9ms/step - loss: 0.7644 - acc: 0.7261 - val_loss: 1.0510 - val_acc: 0.6373\n",
      "Epoch 3/12\n",
      "1530/1530 [==============================] - 19s 13ms/step - loss: 0.5639 - acc: 0.8163 - val_loss: 0.6199 - val_acc: 0.7608\n",
      "Epoch 4/12\n",
      "1530/1530 [==============================] - 18s 12ms/step - loss: 0.4055 - acc: 0.8725 - val_loss: 0.5653 - val_acc: 0.7863\n",
      "Epoch 5/12\n",
      "1530/1530 [==============================] - 17s 11ms/step - loss: 0.3121 - acc: 0.9105 - val_loss: 0.7196 - val_acc: 0.7686\n",
      "Epoch 6/12\n",
      "1530/1530 [==============================] - 26s 17ms/step - loss: 0.2686 - acc: 0.9222 - val_loss: 0.5448 - val_acc: 0.8078\n",
      "Epoch 7/12\n",
      "1530/1530 [==============================] - 26s 17ms/step - loss: 0.3065 - acc: 0.9000 - val_loss: 0.5110 - val_acc: 0.8353\n",
      "Epoch 8/12\n",
      "1530/1530 [==============================] - 15s 10ms/step - loss: 0.2062 - acc: 0.9392 - val_loss: 0.4211 - val_acc: 0.8529\n",
      "Epoch 9/12\n",
      "1530/1530 [==============================] - 21s 13ms/step - loss: 0.1737 - acc: 0.9490 - val_loss: 0.4142 - val_acc: 0.8529\n",
      "Epoch 10/12\n",
      "1530/1530 [==============================] - 15s 10ms/step - loss: 0.1249 - acc: 0.9654 - val_loss: 0.3515 - val_acc: 0.8686\n",
      "Epoch 11/12\n",
      "1530/1530 [==============================] - 14s 9ms/step - loss: 0.1583 - acc: 0.9549 - val_loss: 0.4763 - val_acc: 0.8275\n",
      "Epoch 12/12\n",
      "1530/1530 [==============================] - 18s 12ms/step - loss: 0.1306 - acc: 0.9595 - val_loss: 0.4326 - val_acc: 0.8529\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "  x_train, x_test, y_train, y_test = train_test_split(x_r, y_r, test_size=0.25)\n",
    "\n",
    "  model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.adam(lr = 0.001), metrics=['accuracy'])\n",
    "  print(model.summary())\n",
    "  model.fit(x_train, y_train, batch_size=40, epochs=12, verbose=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, input_size, output_size, activation=\"None\", weights=None):\n",
    "        \"\"\"\n",
    "        :param  input_size: Length of input vector\n",
    "        :param output_size: Length of output vector\n",
    "        :param  activation: Activation function (\"relu\", \"softmax\", \"sigmoid\", \"None\")\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        # Will be used for caching\n",
    "        self.last_input = None\n",
    "        self.last_weighted = None\n",
    "        self.last_input_shape = None\n",
    "        if weights is None:\n",
    "            self.weights = np.random.randn(self.input_size, self.output_size) / self.input_size  # Might need to adjust\n",
    "        else:\n",
    "            # Custom weights needs to be same size as specified\n",
    "            assert (weights.shape[0] == self.input_size and weights.shape[1] == self.output_size)\n",
    "            self.weights = weights\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.last_input_shape = X.shape  # Caching last input shape\n",
    "        X = X.flatten()  # Auto flattening just in case\n",
    "        self.last_input = X  # Caching last input, Flattened\n",
    "        assert (len(self.last_input) == self.input_size)  # Check if shape was correct\n",
    "\n",
    "        weighted_total = np.dot(X, self.weights)\n",
    "        self.last_weighted = weighted_total  # Caching weighted inputs before activation\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return sigmoid(weighted_total)\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            res = weighted_total\n",
    "            return res * (res > 0)  # applying Relu\n",
    "\n",
    "        if self.activation == \"softmax\":\n",
    "            # e = np.exp(X - np.max(X))  # prevent overflow\n",
    "            e = np.exp(weighted_total)\n",
    "            return e / np.sum(e, axis=0)\n",
    "        # Activation doesn't exist or None specified\n",
    "        return weighted_total\n",
    "\n",
    "    def _no_activation_backprop(self, dH, lr):\n",
    "        self.weights -= lr * (self.last_input[np.newaxis].T @ dH[np.newaxis])\n",
    "        dL_dx = np.dot(dH, self.weights.T)\n",
    "        return dL_dx.reshape(self.last_input_shape)\n",
    "\n",
    "    def backward(self, dH, lr=0.01):\n",
    "        \"\"\"\n",
    "            dH is loss differentiated by output (dL/dO). in case of softmax:\n",
    "            loss is cross entropy loss -ln(output[C]) where C is the correct label;\n",
    "            hence, it differentiated by output would be 0 for every label other than C\n",
    "            and -1/output(C) for the C\n",
    "        \"\"\"\n",
    "        if self.activation == \"None\":\n",
    "            return self._no_activation_backprop(dH, lr)\n",
    "\n",
    "        # Only implemented for softmax now\n",
    "        if self.activation != \"softmax\": assert ()\n",
    "\n",
    "        for i, gradient in enumerate(dH):\n",
    "            # Looking for non-zero value (correct label corresponding one)\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "\n",
    "            # e^totals\n",
    "            e_t = np.exp(self.last_weighted)\n",
    "\n",
    "            # Sum of all e^totals\n",
    "            S = np.sum(e_t)\n",
    "\n",
    "            # Gradients of out[i] against totals\n",
    "            do_dt = -e_t[i] * e_t / (S ** 2)\n",
    "            # special derivative case for correct label\n",
    "            do_dt[i] = e_t[i] * (S - e_t[i]) / (S ** 2)\n",
    "\n",
    "            # we need to calculate loss derivative against weights\n",
    "            # and update weights, dL/dw = dL/dO*dO/dt*dt/dw\n",
    "            # we need to differentiate equation t = X*W\n",
    "            dt_dw = self.last_input\n",
    "            dt_dx = self.weights\n",
    "\n",
    "            # Gradient of loss against total dL/dt = dL/dO*dO/dt\n",
    "            dL_dt = gradient * do_dt\n",
    "\n",
    "            # Gradient of loss against weights and input\n",
    "            # dL/dw = dL/dt * dt/dw; dL/dx = dL/dt * dt/dx;\n",
    "            # Temporarily adding axis to multiply matrices and get weight sized matrix\n",
    "            # (input_size, 1)*(1, output_size) = (input_size, output_size)\n",
    "            dL_dw = dt_dw[np.newaxis].T @ dL_dt[np.newaxis]\n",
    "            # (input_size, output_size)*(output_size, 1) = input_size\n",
    "            dL_dx = dt_dx @ dL_dt\n",
    "\n",
    "            # Update weights\n",
    "            self.weights -= lr * dL_dw\n",
    "\n",
    "            return dL_dx.reshape(self.last_input_shape)  # return to input dimensions\n",
    "\n",
    "\n",
    "# A.k.a. Kernel\n",
    "class Filter:\n",
    "    def __init__(self, filter_dims, matrix_dims, filter_id):\n",
    "        \"\"\"\n",
    "        :param filter_dims: 2 dimensional tuple, specifying filter dimensions\n",
    "        :param matrix_dims: takes 2D array as matrix size, or 3D array, where channels are last dimension\n",
    "        \"\"\"\n",
    "        self.filter_id = filter_id\n",
    "        self.is_3d = len(matrix_dims) == 3\n",
    "        self.dims = filter_dims\n",
    "        self.matrix_dims = matrix_dims\n",
    "        # initializing filter randomly\n",
    "        self.filter = np.random.randn(*filter_dims)\n",
    "        # output matrix after filtering is done\n",
    "        self.output_matrix_dims = \\\n",
    "            (matrix_dims[0] - filter_dims[0] + 1, matrix_dims[1] - filter_dims[1] + 1)\n",
    "\n",
    "    def _filter_region_iteration(self, matrix):\n",
    "        \"\"\"\n",
    "        Generate filter sized grids from original matrix, to fill output matrix\n",
    "        if matrix (input_shape[0] - filter_size[0] + 1, input_shape[1] - filter_size[1] + 1)contains channels return region from one channel at a time\n",
    "        :param matrix: Matrix for which to return regions\n",
    "        :return: filter sized cut-out matrix for which we should perform\n",
    "                element-wise multiplication and summing\n",
    "        \"\"\"\n",
    "        for x in range(self.output_matrix_dims[0]):\n",
    "            for y in range(self.output_matrix_dims[1]):\n",
    "                region = np.sum(matrix[x: (x + self.dims[0]), y: (y + self.dims[1])], axis=2) if self.is_3d \\\n",
    "                    else matrix[x: (x + self.dims[0]), y: (y + self.dims[1])]\n",
    "                yield region, x, y\n",
    "\n",
    "    def forward(self, matrix):\n",
    "        \"\"\"\n",
    "        Apply filter to a given matrix and return output\n",
    "        :param matrix:\n",
    "        :return: Outputs filter applied numpy matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if conditions apply\n",
    "        assert (matrix is not None and self.matrix_dims == matrix.shape)\n",
    "        output = np.zeros(self.output_matrix_dims)\n",
    "        # Fill output (filtered) matrix\n",
    "        for region, x, y in self._filter_region_iteration(matrix):\n",
    "            # Summing elementwise multiplication of filter and region\n",
    "            # Getting new pixel value for output\n",
    "            output[x][y] = np.sum(np.multiply(region, self.filter))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, last_input, dH, learning_rate):\n",
    "        assert (last_input.shape == self.matrix_dims and (dH.shape[:-1] == self.output_matrix_dims\n",
    "                                                          or dH.shape == self.output_matrix_dims))\n",
    "\n",
    "        # dL/dW, updating weights\n",
    "        for im_region, i, j in self._filter_region_iteration(last_input):\n",
    "            # Weights going against the gradient with learning rate penalty\n",
    "            self.filter -= dH[i, j, self.filter_id] * learning_rate * im_region\n",
    "\n",
    "        # Calculating input derivative\n",
    "        dX = np.zeros(last_input.shape)  # Need to return derivative of loss in regards of last input (dL/dX)\n",
    "        for i in range(self.output_matrix_dims[0]):\n",
    "            for j in range(self.output_matrix_dims[1]):\n",
    "                # dL/dX, Even in case of channels (3D last input) vector math resolves itself\n",
    "                inc = np.dot(self.filter, dH[i, j, self.filter_id])\n",
    "                if len(dX.shape) == 3:\n",
    "                    inc = np.stack([inc for _ in range(dX.shape[2])], axis=2)\n",
    "                dX[i: i + self.dims[0], j: j + self.dims[1]] += inc\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "class Convolution2D:\n",
    "    \"\"\"\n",
    "    No activation function for now\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_filters, filter_size, input_shape):\n",
    "        \"\"\"\n",
    "        Convolution layer for cnn network\n",
    "        :param num_filters: number of filters (kernels) to apply to each input\n",
    "        :param filter_size: filter dimensions (kernel_size)\n",
    "        :param input_shape: shape of input matrix\n",
    "        \"\"\"\n",
    "        assert (num_filters > 0 and filter_size and input_shape)\n",
    "        assert (filter_size[0] > 0 and filter_size[1] > 0 and input_shape[0] > 0 and input_shape[1] > 0)\n",
    "        self.num_inputs = input_shape[2] if len(input_shape) == 3 else 1\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_dims = filter_size\n",
    "        self.input_shape = input_shape\n",
    "        self.filters = [Filter(filter_size, input_shape, i) for i in range(num_filters)]\n",
    "\n",
    "        # Cache variables\n",
    "        self.last_input = None\n",
    "\n",
    "    def output_shape(self):\n",
    "        \"\"\" Return output matrix shape after forward passing \"\"\"\n",
    "        return self.input_shape[0] - self.filter_dims[0] + 1, self.input_shape[1] - self.filter_dims[\n",
    "            1] + 1, self.num_filters\n",
    "\n",
    "    def forward(self, matrix):\n",
    "        \"\"\"\n",
    "        Apply filters to the given matrix\n",
    "        :param matrix: Matrix to apply filters to\n",
    "        :return: 3D numpy matrix, channels as last dimension\n",
    "        \"\"\"\n",
    "        # Returning 3D matrix, filtered output layers stacked on top of each other, for each filter, for each matrix\n",
    "        self.last_input = matrix\n",
    "        result = np.array([f.forward(matrix) for f in self.filters])\n",
    "        return result.transpose((1, 2, 0))  # Reversing dimensions putting channels as 3rd dimension\n",
    "\n",
    "    def backward(self, dH, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation of convolutional layer.\n",
    "        - dH is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        \"\"\"\n",
    "        dLdX = np.zeros(self.last_input.shape)\n",
    "\n",
    "        # Updating filter weights and getting input matrix derivative from each filter\n",
    "        # We need to sum those up, since all filters used every channel of input summed\n",
    "        for f in self.filters:\n",
    "            dLdX += f.backward(self.last_input, dH, learning_rate)\n",
    "\n",
    "        # we need to return dL/dX\n",
    "        # the loss gradient for this layer's inputs, just like every\n",
    "        # other layer in our CNN.\n",
    "        return dLdX\n",
    "\n",
    "\n",
    "class MaxPooling2D:\n",
    "    def __init__(self, input_shape, pool_size=2):\n",
    "        self.dim = pool_size\n",
    "        self.num_inputs = input_shape[2] if len(input_shape) == 3 else 1\n",
    "        self.matrix_dims = input_shape\n",
    "        self.output_matrix_dims = self.output_shape()\n",
    "\n",
    "        # Caching variables\n",
    "        self.last_input = None\n",
    "\n",
    "    def _pooling_region_iteration(self, matrix):\n",
    "        \"\"\"\n",
    "        Generate pooling sized grids from original matrix, to fill output matrix\n",
    "        if matrix contains channels return region with every channel, and maximize on first twos\n",
    "        :param matrix: Matrix for which to return regions\n",
    "        :return: filter sized cut-out matrix on which we should perform\n",
    "                max operation\n",
    "        \"\"\"\n",
    "        for x in range(self.output_matrix_dims[0]):\n",
    "            for y in range(self.output_matrix_dims[1]):\n",
    "                region = matrix[(x * self.dim):((x + 1) * self.dim), (y * self.dim):((y + 1) * self.dim)]\n",
    "                # Getting poolsized region\n",
    "                yield region, x, y\n",
    "\n",
    "    def output_shape(self):\n",
    "        return self.matrix_dims[0] // self.dim, self.matrix_dims[1] // self.dim, self.num_inputs\n",
    "\n",
    "    def forward(self, matrix):\n",
    "        \"\"\"\n",
    "        Apply max-pooling to a given matrix and return output\n",
    "        :param matrix:\n",
    "        :return: Outputs max-polled numpy matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if conditions apply\n",
    "        assert (matrix is not None and self.matrix_dims == matrix.shape)\n",
    "        # Caching\n",
    "        self.last_input = matrix\n",
    "\n",
    "        output = np.zeros(self.output_matrix_dims)\n",
    "\n",
    "        for region, x, y in self._pooling_region_iteration(matrix):\n",
    "            # Getting max from elements, third dimension isn't changed\n",
    "            # Getting new pixel value for output\n",
    "            output[x][y] = np.amax(region, axis=(0, 1))  # performing argmax on axis 0 and 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dH):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation of the MaxPooling2D layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - dH is the loss gradient for this layer's outputs.\n",
    "        \"\"\"\n",
    "        dLdX = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for region, i, j in self._pooling_region_iteration(self.last_input):\n",
    "            height, width, channels = region.shape\n",
    "            max_value = np.amax(region, axis=(0, 1))\n",
    "\n",
    "            for h in range(height):\n",
    "                for w in range(width):\n",
    "                    for c in range(channels):\n",
    "                        # If this pixel was the max value, copy the gradient in it.\n",
    "                        if region[h, w, c] == max_value[c]:\n",
    "                            dLdX[i * 2 + h, j * 2 + w, c] = dH[i, j, c]\n",
    "\n",
    "        return dLdX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        print(\"Model 1\")\n",
    "        self.num_classes = num_classes\n",
    "        kernel_size = (3, 3)\n",
    "        self.conv1 = Convolution2D(8, kernel_size, input_shape)\n",
    "        self.pooling1 = MaxPooling2D(self.conv1.output_shape(), 2)\n",
    "        flattened_size = self.pooling1.output_shape()[0] * self.pooling1.output_shape()[1] * \\\n",
    "                         self.pooling1.output_shape()[2]\n",
    "        self.dense1 = Dense(flattened_size, num_classes, activation='softmax')\n",
    "\n",
    "\n",
    "    def train(self, X, y, X_test, y_test, lr, epochs):\n",
    "        assert (len(X) == len(y))\n",
    "        print(\"----------- CNN training started! ------------\")\n",
    "        for ep_i in range(epochs):\n",
    "            print(\"Starting epoch {}\".format(ep_i))\n",
    "            print(\"Shuffling training data at the start of the epoch.\")\n",
    "            # Giving same permutation to the X and y before each epoch\n",
    "            perm = np.random.permutation(len(y))\n",
    "            X = X[perm]\n",
    "            y = y[perm]\n",
    "            loss_in_cluster = 0\n",
    "            accuracy_in_cluster = 0\n",
    "            epoch_accuracy = 0\n",
    "            epoch_loss = 0\n",
    "            counter = 0\n",
    "            for x_i, y_i in zip(X, y):\n",
    "                y_i = np.argmax(y_i) # setting y_i as a single label from array\n",
    "                counter += 1\n",
    "                if counter % 200 == 0:\n",
    "                    print(\"|Instance {}| Last 200 Instances: Average loss {} / Accuracy {}%\"\n",
    "                          .format(counter, loss_in_cluster/200, accuracy_in_cluster/2))\n",
    "                    accuracy_in_cluster = 0\n",
    "                    loss_in_cluster = 0\n",
    "                # Perform forward passing\n",
    "                output, loss, accuracy = self.forward(x_i, y_i)\n",
    "\n",
    "                # Updating accuracy\n",
    "                accuracy_in_cluster += accuracy\n",
    "                epoch_accuracy += accuracy\n",
    "                # Updating Loss\n",
    "                loss_in_cluster += loss\n",
    "                epoch_loss += loss\n",
    "                # Gradient for softmax\n",
    "                gradient = np.zeros(self.num_classes)  # Only works when last layer is softmax\n",
    "                gradient[y_i] = -1 / output[y_i]  # Gradient of cross-entropy loss\n",
    "                # Performing backpropagation\n",
    "                self.backward(gradient, lr)\n",
    "            epoch_loss /= len(X)\n",
    "            epoch_accuracy /= len(X)\n",
    "            print(\"-- Epoch {} finished with: Average Loss: {} / Accuracy: {}%;\".format(ep_i, epoch_loss, 100*epoch_accuracy))\n",
    "            self.test(X_test, y_test)\n",
    "    def forward(self, x, label):\n",
    "        output = self.conv1.forward(x)  # Need normalization probably\n",
    "        output = self.pooling1.forward(output)\n",
    "        output = self.dense1.forward(output)\n",
    "\n",
    "        loss = -np.log(output[label])\n",
    "        accuracy = 1 if np.argmax(output) == label else 0\n",
    "\n",
    "        return output, loss, accuracy\n",
    "\n",
    "    def backward(self, gradient, lr):\n",
    "        gradient = self.dense1.backward(gradient, lr)\n",
    "        # Reshaping should be handled by dense layer\n",
    "        gradient = self.pooling1.backward(gradient)\n",
    "        gradient = self.conv1.backward(gradient, lr)\n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        assert(len(X_test) == len(y_test))\n",
    "        print(\"------------ Starting model testing -----------\")\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        predicted_labels = []\n",
    "        for x_i, y_i in zip(X_test, y_test):\n",
    "            y_i = np.argmax(y_i)\n",
    "            pred, ls, acc = self.forward(x_i, y_i)\n",
    "            loss += ls\n",
    "            correct += acc\n",
    "            predicted_labels.append(pred)\n",
    "        res = (predicted_labels, loss/len(X_test), 100*correct/len(X_test))\n",
    "        print(\"--------------- Test Finished: Loss {} | Accuracy {}% --------------\".format(res[1], res[2]))\n",
    "        return res\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        print(\"Prediction starting...\")\n",
    "        cl = [0 for _ in range(self.num_classes)]\n",
    "        cl[0] = 1\n",
    "        dummy_y = np.array([cl for _ in range(len(X_test))])\n",
    "        prediction, _, _ = self.test(X_test, dummy_y)\n",
    "        return prediction\n",
    "        \n",
    "    def export_trained_model(self, dir_path):\n",
    "        np.save(\"{}/fully_connected.npy\".format(dir_path), self.dense1.weights)\n",
    "        i = 0\n",
    "        for flt in self.conv1.filters:\n",
    "            np.save(\"{}/filter{}.npy\".format(dir_path, i), flt.filter)\n",
    "            i += 1\n",
    "\n",
    "    def import_trained_model(self, dir_path):\n",
    "        self.dense1.weights = np.load(\"{}/fully_connected.npy\".format(dir_path))\n",
    "        i = 0\n",
    "        for flt in self.conv1.filters:\n",
    "            flt.filter = np.load(\"{}/filter{}.npy\".format(dir_path, i))\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "----------- CNN training started! ------------\n",
      "Starting epoch 0\n",
      "Shuffling training data at the start of the epoch.\n",
      "|Instance 200| Last 200 Instances: Average loss 2.9761005789757315 / Accuracy 36.0%\n",
      "|Instance 400| Last 200 Instances: Average loss 2.0608953272907127 / Accuracy 45.0%\n",
      "|Instance 600| Last 200 Instances: Average loss 1.6220457842044522 / Accuracy 53.5%\n",
      "|Instance 800| Last 200 Instances: Average loss 1.5627447338689904 / Accuracy 59.0%\n",
      "|Instance 1000| Last 200 Instances: Average loss 1.5920171432404064 / Accuracy 55.5%\n",
      "|Instance 1200| Last 200 Instances: Average loss 1.1179294693226236 / Accuracy 62.5%\n",
      "|Instance 1400| Last 200 Instances: Average loss 1.221566378915193 / Accuracy 62.0%\n",
      "-- Epoch 0 finished with: Average Loss: 1.6829006259726844 / Accuracy: 54.509803921568626%;\n",
      "------------ Starting model testing -----------\n",
      "--------------- Test Finished: Loss 1.217192896948316 | Accuracy 60.98039215686274% --------------\n",
      "Starting epoch 1\n",
      "Shuffling training data at the start of the epoch.\n",
      "|Instance 200| Last 200 Instances: Average loss 0.8489780886920574 / Accuracy 69.5%\n",
      "|Instance 400| Last 200 Instances: Average loss 1.0051037913322745 / Accuracy 71.5%\n",
      "|Instance 600| Last 200 Instances: Average loss 0.9257285622067338 / Accuracy 71.0%\n",
      "|Instance 800| Last 200 Instances: Average loss 0.9977241105132076 / Accuracy 66.5%\n",
      "|Instance 1000| Last 200 Instances: Average loss 0.9697466838903552 / Accuracy 66.5%\n",
      "|Instance 1200| Last 200 Instances: Average loss 1.0080815069444853 / Accuracy 66.5%\n",
      "|Instance 1400| Last 200 Instances: Average loss 0.9385489305305724 / Accuracy 64.0%\n",
      "-- Epoch 1 finished with: Average Loss: 0.9483345609427246 / Accuracy: 68.16993464052288%;\n",
      "------------ Starting model testing -----------\n",
      "--------------- Test Finished: Loss 0.8575323715014455 | Accuracy 70.3921568627451% --------------\n",
      "Starting epoch 2\n",
      "Shuffling training data at the start of the epoch.\n",
      "|Instance 200| Last 200 Instances: Average loss 0.7389435215311522 / Accuracy 76.0%\n",
      "|Instance 400| Last 200 Instances: Average loss 0.7414878696948688 / Accuracy 73.0%\n",
      "|Instance 600| Last 200 Instances: Average loss 0.5694580736172616 / Accuracy 80.0%\n",
      "|Instance 800| Last 200 Instances: Average loss 0.7027736848567185 / Accuracy 71.5%\n",
      "|Instance 1000| Last 200 Instances: Average loss 0.6915214928979749 / Accuracy 74.5%\n",
      "|Instance 1200| Last 200 Instances: Average loss 0.8761567701022567 / Accuracy 68.0%\n",
      "|Instance 1400| Last 200 Instances: Average loss 0.845487959622187 / Accuracy 71.0%\n",
      "-- Epoch 2 finished with: Average Loss: 0.7565637314294908 / Accuracy: 72.94117647058823%;\n",
      "------------ Starting model testing -----------\n",
      "--------------- Test Finished: Loss 0.9415338228883781 | Accuracy 66.86274509803921% --------------\n",
      "Starting epoch 3\n",
      "Shuffling training data at the start of the epoch.\n",
      "|Instance 200| Last 200 Instances: Average loss 0.4787764031914833 / Accuracy 82.0%\n",
      "|Instance 400| Last 200 Instances: Average loss 0.5794143250780599 / Accuracy 78.0%\n",
      "|Instance 600| Last 200 Instances: Average loss 0.7314981811386425 / Accuracy 75.0%\n",
      "|Instance 800| Last 200 Instances: Average loss 0.6057286843646861 / Accuracy 81.5%\n",
      "|Instance 1000| Last 200 Instances: Average loss 0.6354754567384335 / Accuracy 75.0%\n",
      "|Instance 1200| Last 200 Instances: Average loss 0.6228567182900321 / Accuracy 77.0%\n",
      "|Instance 1400| Last 200 Instances: Average loss 0.7416270930287454 / Accuracy 75.0%\n",
      "-- Epoch 3 finished with: Average Loss: 0.6473440327262145 / Accuracy: 76.9281045751634%;\n",
      "------------ Starting model testing -----------\n"
     ]
    }
   ],
   "source": [
    "x_r = (x_r - np.mean(x_r))/np.std(x_r)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_r, y_r, test_size=0.25)\n",
    "custom_model = SequentialModel(input_shape, num_classes)\n",
    "custom_model.train(x_train, y_train, x_test, y_test, 0.004, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.export_trained_model(\"custom_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
