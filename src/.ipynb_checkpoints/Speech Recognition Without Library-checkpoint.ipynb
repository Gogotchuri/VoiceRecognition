{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import librosa\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, LSTM\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     file_name  correct\n",
      "0                ../data_cut/zurab-khut-2b.wav        2\n",
      "1                 ../data_cut/elene-krav-1.wav        1\n",
      "2           ../data_cut/giorgi-chak-3bhigh.wav        3\n",
      "3                 ../data_cut/nika-onia-4a.wav        4\n",
      "4                 ../data_cut/l-andg-2blow.wav        2\n",
      "5                    ../data_cut/l-andg-4d.wav        4\n",
      "6             ../data_cut/zurab-khut-5clow.wav        5\n",
      "7              ../data_cut/devi-khos-1alow.wav        1\n",
      "8              ../data_cut/rezi-mesh-2clow.wav        2\n",
      "9                    ../data_cut/l-andg-4e.wav        4\n",
      "10               ../data_cut/tamar-bedi-4b.wav        4\n",
      "11               ../data_cut/levan-gela-5a.wav        5\n",
      "12           ../data_cut/zurab-khut-1bhigh.wav        1\n",
      "13            ../data_cut/levan-gela-4alow.wav        4\n",
      "14             ../data_cut/devi-khos-1elow.wav        1\n",
      "15        ../data_cut/anamaria-chkh-1-clow.wav        1\n",
      "16                ../data_cut/nika-onia-2e.wav        2\n",
      "17           ../data_cut/giorgi-chak-3clow.wav        3\n",
      "18               ../data_cut/zurab-khut-4c.wav        4\n",
      "19           ../data_cut/demetre-pipi-1low.wav        1\n",
      "20            ../data_cut/levan-gela-1hlow.wav        1\n",
      "21            ../data_cut/revaz-lobz-3blow.wav        3\n",
      "22                 ../data_cut/nata-khur-1.wav        1\n",
      "23        ../data_cut/aleksandre-pert-3low.wav        3\n",
      "24           ../data_cut/tamar-bedi-3ghigh.wav        3\n",
      "25            ../data_cut/revaz-lobz-4dlow.wav        4\n",
      "26        ../data_cut/anamaria-chkh-3-alow.wav        3\n",
      "27             ../data_cut/demetre-pipi-3d.wav        3\n",
      "28             ../data_cut/elene-krav-2low.wav        2\n",
      "29    ../data_cut/giorgi-kala-5-normalhigh.wav        5\n",
      "...                                        ...      ...\n",
      "2010          ../data_cut/zurab-khut-1blow.wav        1\n",
      "2011           ../data_cut/gega-dara-3dlow.wav        3\n",
      "2012            ../data_cut/nanuka-altu-4c.wav        4\n",
      "2013             ../data_cut/lasha-kiti-5e.wav        5\n",
      "2014     ../data_cut/aleksandre-pert-5glow.wav        5\n",
      "2015         ../data_cut/Mariam-chkh-2elow.wav        2\n",
      "2016        ../data_cut/Mariam-chkh-1ahigh.wav        1\n",
      "2017    ../data_cut/aleksandre-pert-4dhigh.wav        4\n",
      "2018     ../data_cut/anamaria-chkh-5-bhigh.wav        5\n",
      "2019             ../data_cut/revaz-lobz-2f.wav        2\n",
      "2020          ../data_cut/vakho-koto-5blow.wav        5\n",
      "2021                 ../data_cut/l-andg-2e.wav        2\n",
      "2022           ../data_cut/demetre-pipi-2d.wav        2\n",
      "2023          ../data_cut/gega-dara-4bhigh.wav        4\n",
      "2024             ../data_cut/zviad-noza-3c.wav        3\n",
      "2025          ../data_cut/zviad-noza-5blow.wav        5\n",
      "2026         ../data_cut/revaz-lobz-4dhigh.wav        4\n",
      "2027             ../data_cut/vakho-koto-4c.wav        4\n",
      "2028        ../data_cut/aleksandre-pert-2b.wav        2\n",
      "2029           ../data_cut/demetre-pipi-5i.wav        5\n",
      "2030             ../data_cut/levan-gela-4h.wav        4\n",
      "2031    ../data_cut/aleksandre-surg-4chigh.wav        4\n",
      "2032        ../data_cut/demetre-pipi-5clow.wav        5\n",
      "2033         ../data_cut/shota-noza-1chigh.wav        1\n",
      "2034           ../data_cut/devi-khos-4high.wav        4\n",
      "2035          ../data_cut/bakuri-tsut-5low.wav        5\n",
      "2036         ../data_cut/levan-gela-3ahigh.wav        3\n",
      "2037    ../data_cut/aleksandre-surg-5bhigh.wav        5\n",
      "2038        ../data_cut/demetre-pipi-4blow.wav        4\n",
      "2039           ../data_cut/nino-chan-4blow.wav        4\n",
      "\n",
      "[2040 rows x 2 columns]\n",
      "50 analyzed out of 2040\n",
      "100 analyzed out of 2040\n",
      "150 analyzed out of 2040\n",
      "200 analyzed out of 2040\n",
      "250 analyzed out of 2040\n",
      "300 analyzed out of 2040\n",
      "350 analyzed out of 2040\n",
      "400 analyzed out of 2040\n",
      "450 analyzed out of 2040\n",
      "500 analyzed out of 2040\n",
      "550 analyzed out of 2040\n",
      "600 analyzed out of 2040\n",
      "650 analyzed out of 2040\n",
      "700 analyzed out of 2040\n",
      "750 analyzed out of 2040\n",
      "800 analyzed out of 2040\n",
      "850 analyzed out of 2040\n",
      "900 analyzed out of 2040\n",
      "950 analyzed out of 2040\n",
      "1000 analyzed out of 2040\n",
      "1050 analyzed out of 2040\n",
      "1100 analyzed out of 2040\n",
      "1150 analyzed out of 2040\n",
      "1200 analyzed out of 2040\n",
      "1250 analyzed out of 2040\n",
      "1300 analyzed out of 2040\n",
      "1350 analyzed out of 2040\n",
      "1400 analyzed out of 2040\n",
      "1450 analyzed out of 2040\n",
      "1500 analyzed out of 2040\n",
      "1550 analyzed out of 2040\n",
      "1600 analyzed out of 2040\n",
      "1650 analyzed out of 2040\n",
      "1700 analyzed out of 2040\n",
      "1750 analyzed out of 2040\n",
      "1800 analyzed out of 2040\n",
      "1850 analyzed out of 2040\n",
      "1900 analyzed out of 2040\n",
      "1950 analyzed out of 2040\n",
      "2000 analyzed out of 2040\n",
      "Model 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'set' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dd1963d3f984>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0mt_matrices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudios_to_spectograms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m \u001b[0mcustom_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"custom_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_x_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-dd1963d3f984>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, num_classes)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvolution2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mflattened_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-dd1963d3f984>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_filters, filter_size, input_shape)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_filters\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfilter_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilter_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object does not support indexing"
     ]
    }
   ],
   "source": [
    "path_to_validation = '../data_cut/'\n",
    "\n",
    "\n",
    "def audios_to_spectograms(file_names):\n",
    "  # Save different shapes in a set\n",
    "  x, shapes = [], set()\n",
    "\n",
    "  # Enumerate for logging\n",
    "  for indx, audio_file in enumerate(file_names):\n",
    "    # Use mfcc algorithm for spectograms\n",
    "    signal, sampling_rate = librosa.load(audio_file) \n",
    "    matrix = librosa.feature.mfcc(signal, sampling_rate)\n",
    "\n",
    "    x.append(matrix)\n",
    "    shapes.add(matrix.shape)\n",
    "    if (indx+1) % 50 == 0: print('{} analyzed out of {}'.format(indx+1, len(file_names))) # Log progress\n",
    "    \n",
    "  return x, shapes\n",
    "\n",
    "def choose_max_shapes(shapes):\n",
    "  # Iterate over shapes and choose biggest possible width and height\n",
    "  w, h = 0, 0\n",
    "  for shape in shapes:\n",
    "    w = max(w, shape[0])\n",
    "    h = max(h, shape[1])\n",
    "  return w, h\n",
    "\n",
    "def pad_spectogram(matrix):\n",
    "  # Since width is always 20 in mfcc, we only check for height difference\n",
    "  if matrix.shape[1] < def_h:\n",
    "    diff = def_h - matrix.shape[1]\n",
    "    # Append half of the difference in beginning\n",
    "    matrix = np.append(np.zeros((matrix.shape[0], diff//2), dtype=float), matrix, axis=1)\n",
    "    #Append res in the end\n",
    "    matrix = np.append(matrix, np.zeros((matrix.shape[0], diff - diff//2), dtype=float), axis=1)\n",
    "  return matrix\n",
    "\n",
    "# Returns correct int from file name\n",
    "def parse_number(file_path):\n",
    "  return int(''.join(ch for ch in list(file_path) if ch.isdigit()))\n",
    "\n",
    "# Return list of tuples (file_path, correct number)\n",
    "def list_of_audios(dir_path):\n",
    "  arr = glob.glob(dir_path + '*.wav')\n",
    "  random.shuffle(arr) # Shuffled data is better for training\n",
    "  return list(map(lambda x: (x, parse_number(x)), arr))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, input_size, output_size, activation=\"None\", weights=None):\n",
    "        \"\"\"\n",
    "        :param  input_size: Length of input vector\n",
    "        :param output_size: Length of output vector\n",
    "        :param  activation: Activation function (\"relu\", \"softmax\", \"sigmoid\", \"None\")\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        # Will be used for caching\n",
    "        self.last_input = None\n",
    "        self.last_weighted = None\n",
    "        self.last_input_shape = None\n",
    "        if weights is None:\n",
    "            self.weights = np.random.randn(self.input_size, self.output_size) / self.input_size  # Might need to adjust\n",
    "        else:\n",
    "            # Custom weights needs to be same size as specified\n",
    "            assert (weights.shape[0] == self.input_size and weights.shape[1] == self.output_size)\n",
    "            self.weights = weights\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.last_input_shape = X.shape  # Caching last input shape\n",
    "        X = X.flatten()  # Auto flattening just in case\n",
    "        self.last_input = X  # Caching last input, Flattened\n",
    "        assert (len(self.last_input) == self.input_size)  # Check if shape was correct\n",
    "\n",
    "        weighted_total = np.dot(X, self.weights)\n",
    "        self.last_weighted = weighted_total  # Caching weighted inputs before activation\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return sigmoid(weighted_total)\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            res = weighted_total\n",
    "            return res * (res > 0)  # applying Relu\n",
    "\n",
    "        if self.activation == \"softmax\":\n",
    "            # e = np.exp(X - np.max(X))  # prevent overflow\n",
    "            e = np.exp(weighted_total)\n",
    "            return e / np.sum(e, axis=0)\n",
    "        # Activation doesn't exist or None specified\n",
    "        return weighted_total\n",
    "\n",
    "    def _no_activation_backprop(self, dH, lr):\n",
    "        self.weights -= lr * (self.last_input[np.newaxis].T @ dH[np.newaxis])\n",
    "        dL_dx = np.dot(dH, self.weights.T)\n",
    "        return dL_dx.reshape(self.last_input_shape)\n",
    "\n",
    "    def backward(self, dH, lr=0.01):\n",
    "        \"\"\"\n",
    "            dH is loss differentiated by output (dL/dO). in case of softmax:\n",
    "            loss is cross entropy loss -ln(output[C]) where C is the correct label;\n",
    "            hence, it differentiated by output would be 0 for every label other than C\n",
    "            and -1/output(C) for the C\n",
    "        \"\"\"\n",
    "        if self.activation == \"None\":\n",
    "            return self._no_activation_backprop(dH, lr)\n",
    "\n",
    "        # Only implemented for softmax now\n",
    "        if self.activation != \"softmax\": assert ()\n",
    "\n",
    "        for i, gradient in enumerate(dH):\n",
    "            # Looking for non-zero value (correct label corresponding one)\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "\n",
    "            # e^totals\n",
    "            e_t = np.exp(self.last_weighted)\n",
    "\n",
    "            # Sum of all e^totals\n",
    "            S = np.sum(e_t)\n",
    "\n",
    "            # Gradients of out[i] against totals\n",
    "            do_dt = -e_t[i] * e_t / (S ** 2)\n",
    "            # special derivative case for correct label\n",
    "            do_dt[i] = e_t[i] * (S - e_t[i]) / (S ** 2)\n",
    "\n",
    "            # we need to calculate loss derivative against weights\n",
    "            # and update weights, dL/dw = dL/dO*dO/dt*dt/dw\n",
    "            # we need to differentiate equation t = X*W\n",
    "            dt_dw = self.last_input\n",
    "            dt_dx = self.weights\n",
    "\n",
    "            # Gradient of loss against total dL/dt = dL/dO*dO/dt\n",
    "            dL_dt = gradient * do_dt\n",
    "\n",
    "            # Gradient of loss against weights and input\n",
    "            # dL/dw = dL/dt * dt/dw; dL/dx = dL/dt * dt/dx;\n",
    "            # Temporarily adding axis to multiply matrices and get weight sized matrix\n",
    "            # (input_size, 1)*(1, output_size) = (input_size, output_size)\n",
    "            dL_dw = dt_dw[np.newaxis].T @ dL_dt[np.newaxis]\n",
    "            # (input_size, output_size)*(output_size, 1) = input_size\n",
    "            dL_dx = dt_dx @ dL_dt\n",
    "\n",
    "            # Update weights\n",
    "            self.weights -= lr * dL_dw\n",
    "\n",
    "            return dL_dx.reshape(self.last_input_shape)  # return to input dimensions\n",
    "\n",
    "\n",
    "# A.k.a. Kernel\n",
    "class Filter:\n",
    "    def __init__(self, filter_dims, matrix_dims, filter_id):\n",
    "        \"\"\"\n",
    "        :param filter_dims: 2 dimensional tuple, specifying filter dimensions\n",
    "        :param matrix_dims: takes 2D array as matrix size, or 3D array, where channels are last dimension\n",
    "        \"\"\"\n",
    "        self.filter_id = filter_id\n",
    "        self.is_3d = len(matrix_dims) == 3\n",
    "        self.dims = filter_dims\n",
    "        self.matrix_dims = matrix_dims\n",
    "        # initializing filter randomly\n",
    "        self.filter = np.random.randn(*filter_dims)\n",
    "        # output matrix after filtering is done\n",
    "        self.output_matrix_dims = \\\n",
    "            (matrix_dims[0] - filter_dims[0] + 1, matrix_dims[1] - filter_dims[1] + 1)\n",
    "\n",
    "    def _filter_region_iteration(self, matrix):\n",
    "        \"\"\"\n",
    "        Generate filter sized grids from original matrix, to fill output matrix\n",
    "        if matrix (input_shape[0] - filter_size[0] + 1, input_shape[1] - filter_size[1] + 1)contains channels return region from one channel at a time\n",
    "        :param matrix: Matrix for which to return regions\n",
    "        :return: filter sized cut-out matrix for which we should perform\n",
    "                element-wise multiplication and summing\n",
    "        \"\"\"\n",
    "        for x in range(self.output_matrix_dims[0]):\n",
    "            for y in range(self.output_matrix_dims[1]):\n",
    "                region = np.sum(matrix[x: (x + self.dims[0]), y: (y + self.dims[1])], axis=2) if self.is_3d \\\n",
    "                    else matrix[x: (x + self.dims[0]), y: (y + self.dims[1])]\n",
    "                yield region, x, y\n",
    "\n",
    "    def forward(self, matrix):\n",
    "        \"\"\"\n",
    "        Apply filter to a given matrix and return output\n",
    "        :param matrix:\n",
    "        :return: Outputs filter applied numpy matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if conditions apply\n",
    "        assert (matrix is not None and self.matrix_dims == matrix.shape)\n",
    "        output = np.zeros(self.output_matrix_dims)\n",
    "        # Fill output (filtered) matrix\n",
    "        for region, x, y in self._filter_region_iteration(matrix):\n",
    "            # Summing elementwise multiplication of filter and region\n",
    "            # Getting new pixel value for output\n",
    "            output[x][y] = np.sum(np.multiply(region, self.filter))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, last_input, dH, learning_rate):\n",
    "        assert (last_input.shape == self.matrix_dims and (dH.shape[:-1] == self.output_matrix_dims\n",
    "                                                          or dH.shape == self.output_matrix_dims))\n",
    "\n",
    "        # dL/dW, updating weights\n",
    "        for im_region, i, j in self._filter_region_iteration(last_input):\n",
    "            # Weights going against the gradient with learning rate penalty\n",
    "            self.filter -= dH[i, j, self.filter_id] * learning_rate * im_region\n",
    "\n",
    "        # Calculating input derivative\n",
    "        dX = np.zeros(last_input.shape)  # Need to return derivative of loss in regards of last input (dL/dX)\n",
    "        for i in range(self.output_matrix_dims[0]):\n",
    "            for j in range(self.output_matrix_dims[1]):\n",
    "                # dL/dX, Even in case of channels (3D last input) vector math resolves itself\n",
    "                inc = np.dot(self.filter, dH[i, j, self.filter_id])\n",
    "                if len(dX.shape) == 3:\n",
    "                    inc = np.stack([inc for _ in range(dX.shape[2])], axis=2)\n",
    "                dX[i: i + self.dims[0], j: j + self.dims[1]] += inc\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "class Convolution2D:\n",
    "    \"\"\"\n",
    "    No activation function for now\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_filters, filter_size, input_shape):\n",
    "        \"\"\"\n",
    "        Convolution layer for cnn network\n",
    "        :param num_filters: number of filters (kernels) to apply to each input\n",
    "        :param filter_size: filter dimensions (kernel_size)\n",
    "        :param input_shape: shape of input matrix\n",
    "        \"\"\"\n",
    "        assert (num_filters > 0 and filter_size and input_shape)\n",
    "        assert (filter_size[0] > 0 and filter_size[1] > 0 and input_shape[0] > 0 and input_shape[1] > 0)\n",
    "        self.num_inputs = input_shape[2] if len(input_shape) == 3 else 1\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_dims = filter_size\n",
    "        self.input_shape = input_shape\n",
    "        self.filters = [Filter(filter_size, input_shape, i) for i in range(num_filters)]\n",
    "\n",
    "        # Cache variables\n",
    "        self.last_input = None\n",
    "\n",
    "    def output_shape(self):\n",
    "        \"\"\" Return output matrix shape after forward passing \"\"\"\n",
    "        return self.input_shape[0] - self.filter_dims[0] + 1, self.input_shape[1] - self.filter_dims[\n",
    "            1] + 1, self.num_filters\n",
    "\n",
    "    def forward(self, matrix):\n",
    "        \"\"\"\n",
    "        Apply filters to the given matrix\n",
    "        :param matrix: Matrix to apply filters to\n",
    "        :return: 3D numpy matrix, channels as last dimension\n",
    "        \"\"\"\n",
    "        # Returning 3D matrix, filtered output layers stacked on top of each other, for each filter, for each matrix\n",
    "        self.last_input = matrix\n",
    "        result = np.array([f.forward(matrix) for f in self.filters])\n",
    "        return result.transpose((1, 2, 0))  # Reversing dimensions putting channels as 3rd dimension\n",
    "\n",
    "    def backward(self, dH, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation of convolutional layer.\n",
    "        - dH is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        \"\"\"\n",
    "        dLdX = np.zeros(self.last_input.shape)\n",
    "\n",
    "        # Updating filter weights and getting input matrix derivative from each filter\n",
    "        # We need to sum those up, since all filters used every channel of input summed\n",
    "        for f in self.filters:\n",
    "            dLdX += f.backward(self.last_input, dH, learning_rate)\n",
    "\n",
    "        # we need to return dL/dX\n",
    "        # the loss gradient for this layer's inputs, just like every\n",
    "        # other layer in our CNN.\n",
    "        return dLdX\n",
    "\n",
    "\n",
    "class MaxPooling2D:\n",
    "    def __init__(self, input_shape, pool_size=2):\n",
    "        self.dim = pool_size\n",
    "        self.num_inputs = input_shape[2] if len(input_shape) == 3 else 1\n",
    "        self.matrix_dims = input_shape\n",
    "        self.output_matrix_dims = self.output_shape()\n",
    "\n",
    "        # Caching variables\n",
    "        self.last_input = None\n",
    "\n",
    "    def _pooling_region_iteration(self, matrix):\n",
    "        \"\"\"\n",
    "        Generate pooling sized grids from original matrix, to fill output matrix\n",
    "        if matrix contains channels return region with every channel, and maximize on first twos\n",
    "        :param matrix: Matrix for which to return regions\n",
    "        :return: filter sized cut-out matrix on which we should perform\n",
    "                max operation\n",
    "        \"\"\"\n",
    "        for x in range(self.output_matrix_dims[0]):\n",
    "            for y in range(self.output_matrix_dims[1]):\n",
    "                region = matrix[(x * self.dim):((x + 1) * self.dim), (y * self.dim):((y + 1) * self.dim)]\n",
    "                # Getting poolsized region\n",
    "                yield region, x, y\n",
    "\n",
    "    def output_shape(self):\n",
    "        return self.matrix_dims[0] // self.dim, self.matrix_dims[1] // self.dim, self.num_inputs\n",
    "\n",
    "    def forward(self, matrix):\n",
    "        \"\"\"\n",
    "        Apply max-pooling to a given matrix and return output\n",
    "        :param matrix:\n",
    "        :return: Outputs max-polled numpy matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if conditions apply\n",
    "        assert (matrix is not None and self.matrix_dims == matrix.shape)\n",
    "        # Caching\n",
    "        self.last_input = matrix\n",
    "\n",
    "        output = np.zeros(self.output_matrix_dims)\n",
    "\n",
    "        for region, x, y in self._pooling_region_iteration(matrix):\n",
    "            # Getting max from elements, third dimension isn't changed\n",
    "            # Getting new pixel value for output\n",
    "            output[x][y] = np.amax(region, axis=(0, 1))  # performing argmax on axis 0 and 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dH):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation of the MaxPooling2D layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - dH is the loss gradient for this layer's outputs.\n",
    "        \"\"\"\n",
    "        dLdX = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for region, i, j in self._pooling_region_iteration(self.last_input):\n",
    "            height, width, channels = region.shape\n",
    "            max_value = np.amax(region, axis=(0, 1))\n",
    "\n",
    "            for h in range(height):\n",
    "                for w in range(width):\n",
    "                    for c in range(channels):\n",
    "                        # If this pixel was the max value, copy the gradient in it.\n",
    "                        if region[h, w, c] == max_value[c]:\n",
    "                            dLdX[i * 2 + h, j * 2 + w, c] = dH[i, j, c]\n",
    "\n",
    "        return dLdX\n",
    "    \n",
    "    \n",
    "class SequentialModel:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        print(\"Model 1\")\n",
    "        self.num_classes = num_classes\n",
    "        kernel_size = (3, 3)\n",
    "        self.conv1 = Convolution2D(8, kernel_size, input_shape)\n",
    "        self.pooling1 = MaxPooling2D(self.conv1.output_shape(), 2)\n",
    "        flattened_size = self.pooling1.output_shape()[0] * self.pooling1.output_shape()[1] * \\\n",
    "                         self.pooling1.output_shape()[2]\n",
    "        self.dense1 = Dense(flattened_size, num_classes, activation='softmax')\n",
    "\n",
    "\n",
    "    def train(self, X, y, X_test, y_test, lr, epochs):\n",
    "        assert (len(X) == len(y))\n",
    "        print(\"----------- CNN training started! ------------\")\n",
    "        for ep_i in range(epochs):\n",
    "            print(\"Starting epoch {}\".format(ep_i))\n",
    "            print(\"Shuffling training data at the start of the epoch.\")\n",
    "            # Giving same permutation to the X and y before each epoch\n",
    "            perm = np.random.permutation(len(y))\n",
    "            X = X[perm]\n",
    "            y = y[perm]\n",
    "            loss_in_cluster = 0\n",
    "            accuracy_in_cluster = 0\n",
    "            epoch_accuracy = 0\n",
    "            epoch_loss = 0\n",
    "            counter = 0\n",
    "            for x_i, y_i in zip(X, y):\n",
    "                y_i = np.argmax(y_i) # setting y_i as a single label from array\n",
    "                counter += 1\n",
    "                if counter % 200 == 0:\n",
    "                    print(\"|Instance {}| Last 200 Instances: Average loss {} / Accuracy {}%\"\n",
    "                          .format(counter, loss_in_cluster/200, accuracy_in_cluster/2))\n",
    "                    accuracy_in_cluster = 0\n",
    "                    loss_in_cluster = 0\n",
    "                # Perform forward passing\n",
    "                output, loss, accuracy = self.forward(x_i, y_i)\n",
    "\n",
    "                # Updating accuracy\n",
    "                accuracy_in_cluster += accuracy\n",
    "                epoch_accuracy += accuracy\n",
    "                # Updating Loss\n",
    "                loss_in_cluster += loss\n",
    "                epoch_loss += loss\n",
    "                # Gradient for softmax\n",
    "                gradient = np.zeros(self.num_classes)  # Only works when last layer is softmax\n",
    "                gradient[y_i] = -1 / output[y_i]  # Gradient of cross-entropy loss\n",
    "                # Performing backpropagation\n",
    "                self.backward(gradient, lr)\n",
    "            epoch_loss /= len(X)\n",
    "            epoch_accuracy /= len(X)\n",
    "            print(\"-- Epoch {} finished with: Average Loss: {} / Accuracy: {}%;\".format(ep_i, epoch_loss, 100*epoch_accuracy))\n",
    "            self.test(X_test, y_test)\n",
    "    def forward(self, x, label):\n",
    "        output = self.conv1.forward(x)  # Need normalization probably\n",
    "        output = self.pooling1.forward(output)\n",
    "        output = self.dense1.forward(output)\n",
    "\n",
    "        loss = -np.log(output[label])\n",
    "        accuracy = 1 if np.argmax(output) == label else 0\n",
    "\n",
    "        return output, loss, accuracy\n",
    "\n",
    "    def backward(self, gradient, lr):\n",
    "        gradient = self.dense1.backward(gradient, lr)\n",
    "        # Reshaping should be handled by dense layer\n",
    "        gradient = self.pooling1.backward(gradient)\n",
    "        gradient = self.conv1.backward(gradient, lr)\n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        assert(len(X_test) == len(y_test))\n",
    "        print(\"------------ Starting model testing -----------\")\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        predicted_labels = []\n",
    "        for x_i, y_i in zip(X_test, y_test):\n",
    "            y_i = np.argmax(y_i)\n",
    "            pred, ls, acc = self.forward(x_i, y_i)\n",
    "            loss += ls\n",
    "            correct += acc\n",
    "            predicted_labels.append(pred)\n",
    "        res = (predicted_labels, loss/len(X_test), 100*correct/len(X_test))\n",
    "        print(\"--------------- Test Finished: Loss {} | Accuracy {}% --------------\".format(res[1], res[2]))\n",
    "        return res\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        print(\"Prediction starting...\")\n",
    "        dummy_y = np.random.randn(len(X_test), self.num_classes)\n",
    "        prediction, _, _ = self.test(X_test, dummy_y)\n",
    "        return prediction\n",
    "        \n",
    "    def export_trained_model(self, dir_path):\n",
    "        np.save(\"{}/fully_connected.npy\".format(dir_path), self.dense1.weights)\n",
    "        i = 0\n",
    "        for flt in self.conv1.filters:\n",
    "            np.save(\"{}/filter{}.npy\".format(dir_path, i), flt.filter)\n",
    "            i += 1\n",
    "\n",
    "    def import_trained_model(self, dir_path):\n",
    "        self.dense1.weights = np.load(\"{}/fully_connected.npy\".format(dir_path))\n",
    "        i = 0\n",
    "        for flt in self.conv1.filters:\n",
    "            flt.filter = np.load(\"{}/filter{}.npy\".format(dir_path, i))\n",
    "            i += 1\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 50\n",
    "\n",
    "t_df = pd.DataFrame(list_of_audios(path_to_validation), columns = ['file_name', 'correct'])\n",
    "print(t_df)\n",
    "t_matrices, input_shape = audios_to_spectograms(t_df['file_name'])\n",
    "\n",
    "custom_model = SequentialModel(input_shape, num_classes)\n",
    "custom_model.import_trained_model(\"custom_model\")\n",
    "final, prediction = [], custom_model.predict(t_x_r)\n",
    "for i, pred in enumerate(prediction):\n",
    "  final.append([t_df['file_name'][i]] + [np.argmax(pred) + 1] + list(map(lambda s: str(s / sum(pred)), pred)))\n",
    "\n",
    "final_df = pd.DataFrame(final, columns = ['file_name', 'predicted', '1', '2', '3', '4', '5'])\n",
    "print(final_df)\n",
    "final_df.to_csv('from_scratch_results.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
